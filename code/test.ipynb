{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59686884-718e-44fd-af00-70d082691a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json,os, random\n",
    "from  lightgbm import LGBMClassifier,log_evaluation,early_stopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bert4torch.tokenizers import Tokenizer, load_vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffefcd2-dcf5-430e-b3f6-1998a58103e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者ID:{作者姓名：自己的论文（ID形式），错误的论文}\n",
    "with open(\"../raw_data/train_author.json\") as f:\n",
    "    train_author=json.load(f)\n",
    "\n",
    "# 论文ID : titile, 作者姓名，作者单位，期刊，出版年，关键字，摘要\n",
    "with open(\"../raw_data/pid_to_info_all.json\", encoding=\"utf8\") as f:\n",
    "    pid_to_info=json.load(f)\n",
    "    \n",
    "# 作者ID:{作者姓名：所有论文}\n",
    "with open(\"../raw_data/ind_valid_author.json\") as f:\n",
    "    valid_author=json.load(f)\n",
    "    \n",
    "with open(\"../raw_data/ind_valid_author_submit.json\") as f:\n",
    "    submission=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f6ce7-3b35-4e74-8783-a5b408d50e42",
   "metadata": {},
   "source": [
    "# 基本特征采集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba94f7-5b5a-432a-b212-de6e546a10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats=[]\n",
    "labels=[]\n",
    "\n",
    "for e1, (id, person_info) in enumerate(train_author.items()):\n",
    "    for e2, text_id in enumerate(person_info['normal_data']): # 正样本\n",
    "        feat=pid_to_info[text_id]\n",
    "        # if e1 ==0 and e2 == 1:\n",
    "        #     print(feat)\n",
    "        # titile, [作者姓名,作者单位]，摘要, 关键字, 期刊，出版年\n",
    "        # 作者信息这里，数量不一致没法使用\n",
    "        try:\n",
    "            if feat[\"venue\"] == None and feat['year'] == \"\":\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), 0, 2000,\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "\n",
    "            elif feat[\"venue\"] != None and feat['year'] == \"\":\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), len(feat[\"venue\"]), 2000,\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "\n",
    "            elif feat[\"venue\"] == None and feat['year'] != \"\":\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), 0, int(feat['year']),\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "            else:\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), len(feat[\"venue\"]), int(feat['year']),\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "        except:\n",
    "            # pass\n",
    "            print(feat)\n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for text_id in person_info['outliers']:#负样本\n",
    "        feat=pid_to_info[text_id]\n",
    "        try:\n",
    "            if feat[\"venue\"] == None and feat['year'] == \"\":\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), 0, 2000,\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "\n",
    "            elif feat[\"venue\"] != None and feat['year'] == \"\":\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), len(feat[\"venue\"]), 2000,\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "\n",
    "            elif feat[\"venue\"] == None and feat['year'] != \"\":\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), 0, int(feat['year']),\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "            else:\n",
    "                train_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), len(feat[\"venue\"]), int(feat['year']),\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "        except:\n",
    "            # pass\n",
    "            print(feat)\n",
    "        labels.append(0)   \n",
    "\n",
    "valid_feats=[]\n",
    "for id,person_info in valid_author.items():\n",
    "    for text_id in person_info['papers']:\n",
    "        feat=pid_to_info[text_id]\n",
    "        try:\n",
    "            if feat[\"venue\"] == None and feat['year'] == \"\":\n",
    "                valid_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), 0, 2000,\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "\n",
    "            elif feat[\"venue\"] != None and feat['year'] == \"\":\n",
    "                valid_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), len(feat[\"venue\"]), 2000,\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "\n",
    "            elif feat[\"venue\"] == None and feat['year'] != \"\":\n",
    "                valid_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), 0, int(feat['year']),\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "            else:\n",
    "                valid_feats.append([len(feat['title']), len(feat['abstract']), len(feat['keywords']), \n",
    "                     len(feat['authors']), len(feat[\"venue\"]), int(feat['year']),\n",
    "                     str(feat['title']),str(feat['authors']), str(feat['abstract']), feat['keywords'], str(feat[\"venue\"])])\n",
    "        except:\n",
    "            # pass\n",
    "            print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e4399-9e14-4c01-a91b-0e1488aee3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats=pd.DataFrame(train_feats)\n",
    "train_feats.columns = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"title\",\"authors\", \"abstract\", \"keywords\", \"venue\"]\n",
    "train_feats[\"label\"] = labels\n",
    "valid_feats = pd.DataFrame(valid_feats)\n",
    "valid_feats.columns = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"title\",\"authors\",\"abstract\", \"keywords\", \"venue\"]\n",
    "\n",
    "print(f\"train_feats.shape:{train_feats.shape},labels.shape:{np.array(labels).shape}\")\n",
    "print(f\"valid_feats.shape:{valid_feats.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5b740-f533-4044-bc17-eff79d32c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_list_with_nan(value):\n",
    "    if isinstance(value, list) and not value:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "train_feats = train_feats.applymap(replace_empty_list_with_nan)\n",
    "train_feats.replace(\"\", np.nan, inplace=True)\n",
    "train_feats = train_feats.fillna(\"0\")\n",
    "train_feats[\"keywords\"] = train_feats[\"keywords\"].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "valid_feats = valid_feats.applymap(replace_empty_list_with_nan)\n",
    "valid_feats.replace(\"\", np.nan, inplace=True)\n",
    "valid_feats = valid_feats.fillna(\"0\")\n",
    "valid_feats[\"keywords\"] = valid_feats[\"keywords\"].apply(lambda x: ','.join(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c93c3-996a-491a-b5c9-f62be4434345",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a523f-bba6-49a8-bc72-f0102f48a8cb",
   "metadata": {},
   "source": [
    "## 构造文字向量特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7941c62-1ea6-41b8-b13b-59d51696c0a7",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4d469-a692-4419-ba89-91d6d4eeabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7440304-08c3-4feb-a367-4f2133beff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"title\", \"abstract\", \"keywords\", \"venue\"]\n",
    "for e1, col in enumerate(col_list):\n",
    "    if col == \"title\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=5)\n",
    "        \n",
    "    elif col == \"abstract\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=10)\n",
    "        \n",
    "    elif col == \"keywords\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=5)\n",
    "        \n",
    "    elif col == \"venue\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=3)\n",
    "        \n",
    "    tfidfvec.fit(train_feats[col])\n",
    "    new_feats = tfidfvec.transform(train_feats[col])\n",
    "    if e1 == 0:\n",
    "        new_feats_ = new_feats\n",
    "    else:\n",
    "        new_feats_ = sparse.hstack((new_feats_, new_feats))\n",
    "    \n",
    "train_tfidf = pd.DataFrame(new_feats_.toarray())\n",
    "train_tfidf.columns = [\"tfidf_\"+str(i)for i in train_tfidf.columns]\n",
    "train_feats = pd.concat([train_feats, train_tfidf],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "for e1, col in enumerate(col_list):\n",
    "    if col == \"title\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=5)\n",
    "        \n",
    "    elif col == \"abstract\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=10)\n",
    "        \n",
    "    elif col == \"keywords\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=5)\n",
    "        \n",
    "    elif col == \"venue\":\n",
    "        tfidfvec = TfidfVectorizer(\n",
    "    # stop_words=ENGLISH_STOP_WORDS,\n",
    "                          ngram_range=(1,1),\n",
    "                          max_features=3)\n",
    "        \n",
    "    tfidfvec.fit(train_feats[col])\n",
    "    new_feats = tfidfvec.transform(valid_feats[col])\n",
    "    if e1 == 0:\n",
    "        new_feats_ = new_feats\n",
    "    else:\n",
    "        new_feats_ = sparse.hstack((new_feats_, new_feats))\n",
    "        \n",
    "valid_tfidf = pd.DataFrame(new_feats_.toarray())\n",
    "valid_tfidf.columns = [\"tfidf_\"+str(i)for i in valid_tfidf.columns]\n",
    "valid_feats = pd.concat([valid_feats, valid_tfidf],axis=1)\n",
    "    \n",
    "print(f\"train_feats.shape:{train_feats.shape}\")\n",
    "print(f\"valid_feats.shape:{valid_feats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132465a-2720-4082-a8b1-85c70656d84e",
   "metadata": {},
   "source": [
    "### 嵌入特征\n",
    "- 这个方法会为每个单词创建一个向量，所以针对一句话要么将全部单词拼接，要么求平均。但是计算速度很慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b085b9-9ea5-4b99-987e-6533393b791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1e34d-4226-440a-8fbc-461c75026f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    train_feats[\"abstract\"].apply(lambda x:x.split(\" \")),\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "def mean_w2v(x, model, size=5):\n",
    "    i=0\n",
    "    for word in x.split(\" \"):\n",
    "        if word in model.wv.vocab:\n",
    "            i+=1\n",
    "            if i==1:\n",
    "                vec = np.zeros(szie=5)\n",
    "            vec += model.wv[word]\n",
    "    return vec / i\n",
    "    \n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding  = get_mean_w2v(train_feats, \"abstract\", model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932850dd-4ed4-4c4e-af76-8ece6cf6b8f5",
   "metadata": {},
   "source": [
    "### transformer特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8b508-0699-4edd-ab91-d1afbb426c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dir = '../user_data/chinese-bert-wwm-ext/'\n",
    "config_path = pretrained_dir+'config.json'\n",
    "dict_path = pretrained_dir+'vocab.txt'\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "tokenizer_train = []\n",
    "for e1, text in enumerate(train_feats[\"abstract\"].values):\n",
    "    token_ids,_ = tokenizer.encode(text, maxlen=10)\n",
    "    tokenizer_train.append(token_ids)\n",
    "tokenizer_traindf = pd.DataFrame(tokenizer_train)\n",
    "tokenizer_traindf.columns = [\"tokenizer_\"+str(i)for i in tokenizer_traindf.columns]\n",
    "train_feats = pd.concat([train_feats, tokenizer_traindf],axis=1)\n",
    "print(f\"train_feats.shape:{train_feats.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_val = []\n",
    "for e1, text in enumerate(valid_feats[\"abstract\"].values):\n",
    "    token_ids,_ = tokenizer.encode(text, maxlen=10)\n",
    "    tokenizer_val.append(token_ids)\n",
    "    \n",
    "tokenizer_valdf = pd.DataFrame(tokenizer_val)\n",
    "tokenizer_valdf.columns = [\"tokenizer_\"+str(i)for i in tokenizer_valdf.columns]\n",
    "valid_feats = pd.concat([valid_feats, tokenizer_valdf],axis=1)\n",
    "print(f\"valid_feats.shape:{valid_feats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8a9dc-19ad-4dcf-ba89-3bf28f67431e",
   "metadata": {},
   "source": [
    "### CountVectorizer特征\n",
    "- 这个特征和TFIDF差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca5c49-c577-4b7d-bf60-3e70739f327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"name_list\"] = data[\"name_list\"].apply(lambda x : x.replace('[', '').replace(']', '').replace(',', ' '))\n",
    "# data[\"org_list\"] = data[\"org_list\"].apply(lambda x : x.replace('[', '').replace(']', '').replace(',', ' '))\n",
    "\n",
    "# col_list = [\"name_list\", \"org_list\"]\n",
    "# max_features = [3, 3]\n",
    "# new_feats_ = pd.DataFrame()\n",
    "# for e1, col in enumerate(col_list):\n",
    "#     count_vectorizer = CountVectorizer(max_features=max_features[e1] ,token_pattern=r'\\b\\d+\\b')\n",
    "#     count_matrix = pd.DataFrame(count_vectorizer.fit_transform(data[col]).toarray())\n",
    "#     new_feats_ = pd.concat([new_feats_, count_matrix], axis=1)\n",
    "# new_feats_.columns = [\"CountVectorizer_\"+ str(i) for i in range(sum(max_features))]\n",
    "\n",
    "# data = pd.concat([data, new_feats_], axis=1)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f204865-d268-4913-a479-419c57d6968c",
   "metadata": {},
   "source": [
    "## 计算本论文和同作者其他论文的文本相似度\n",
    "### Jaccard相似性、编辑距离、余弦相似度\n",
    "- 编辑距离计算很慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacddd96-0a82-41c4-900f-2fab6c3c44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "nltk.download('punkt')  # 下载必要的分词模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d6da9-3c68-4fbd-a681-70b5a913cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "all_train_num = 0\n",
    "for e1, (id, person_info) in enumerate(train_author.items()):\n",
    "    train_dict[id] = []\n",
    "    train_dict[id].append(len(person_info['normal_data']) + len(person_info['outliers']))\n",
    "    all_train_num += len(person_info['normal_data'])\n",
    "    all_train_num += len(person_info['outliers'])\n",
    "    \n",
    "val_dict = {}   \n",
    "all_val_num = 0\n",
    "for id,person_info in valid_author.items():\n",
    "    val_dict[id] = []\n",
    "    val_dict[id].append(len(person_info['papers']))\n",
    "    all_val_num += len(person_info['papers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5b466-4702-4f4e-8bbc-8242cb98e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jaccard_Similarity_all = []\n",
    "Cosine_Similarity = []\n",
    "\n",
    "for i in range(len(train_data_title)):  #  779个人\n",
    "    print(i)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, max_features=30)\n",
    "    tfidf_matrix = vectorizer.fit_transform(list(train_data_title[i]))\n",
    "    cosine_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    Cosine_Similarity.append(cosine_similarity_matrix)\n",
    "    \n",
    "    jaccard_similarity = []\n",
    "    for e1, j in enumerate(train_data_title[i]):  # 每个人的文章\n",
    "        words1 = set(nltk.word_tokenize(j))\n",
    "        js_value = []\n",
    "        for text1 in train_data_title[i]:\n",
    "            words2 = set(nltk.word_tokenize(text1))\n",
    "            js = 1 - jaccard_distance(words1, words2)\n",
    "            js_value.append(js)\n",
    "        jaccard_similarity.append(js_value)\n",
    "    Jaccard_Similarity_all.append(jaccard_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ab862-8a51-4293-a93f-40e93e1bdf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_js = []\n",
    "for i in range(len(train_data_title)):\n",
    "    for j in range(len(Jaccard_Similarity_all[i])):\n",
    "        df_js.append(pd.Series(Jaccard_Similarity_all[i][j]).mean())\n",
    "\n",
    "df_cs = []\n",
    "for i in range(len(train_data_title)):\n",
    "    for j in range(len(Cosine_Similarity[i])):\n",
    "        df_cs.append(pd.Series(Cosine_Similarity[i][j]).mean())\n",
    "        \n",
    "        \n",
    "yhl = pd.DataFrame([df_js, df_cs]).T\n",
    "display(yhl.head())\n",
    "yhl.to_csv(\"../user_data/title_data.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a1abe-5e66-4d0f-a526-210ae978c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, new_data):\n",
    "    if pd.notna(row['title']):  # 如果 'col1' 列的值不是 NaN\n",
    "        return new_data.pop(0)  # 从 new_values 列表中弹出第一个元素并返回, 只能运行一次\n",
    "    else:\n",
    "        return np.nan  # 如果 'col1' 列的值是 NaN，则在新列中填充 NaN\n",
    "\n",
    "train_data['title_js'] = train_data.apply(process_row, axis=1, args=(df_js,))\n",
    "train_data['title_cs'] = train_data.apply(process_row, axis=1, args=(df_cs,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164896a-3528-4003-b834-1cfff570c71f",
   "metadata": {},
   "source": [
    "## 暴力特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7a340-fd76-49bb-b5cb-47a6fa3a25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "feat_col = ['len(title)', 'len(abstract)', 'len(keywords)', 'len(authors)', 'len(venue)'] + [\n",
    "    \"title_js\", \"title_cs\", \"abs_js\", \"abs_cs\", \"key_js\", \"key_cs\", \"venu_js\"]+[\"year\"]\n",
    "    \n",
    "for i in feat_col:\n",
    "    data[i+\"_cos\"] = np.cos(data[i])\n",
    "    data[i+\"_sin\"] = np.sin(data[i])\n",
    "    data[i+\"_tan\"] = np.tanh(data[i])\n",
    "    for j in feat_col: \n",
    "        if i != j:\n",
    "            data[i+\"*\"+j] = data[i] * data[j]\n",
    "            data[i+\"+\"+j] = data[i] + data[j]\n",
    "            data[i+\"-\"+j] = data[i] - data[j]\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "new_data_featcol = imputer.fit_transform(data[feat_col])\n",
    "new_data_featcol = pd.DataFrame(PolynomialFeatures(degree=3).fit_transform(new_data_featcol))\n",
    "new_data_featcol.columns = [\"Poly\" + str(i) for i in range(new_data_featcol.shape[1])]\n",
    "print(new_data_featcol.shape)\n",
    "\n",
    "data = pd.concat([data, new_data_featcol], axis=1)\n",
    "print(data.shape)\n",
    "\n",
    "with open(\"../raw_data/train_author.json\") as f:\n",
    "    train_author=json.load(f)\n",
    "    \n",
    "with open(\"../raw_data/ind_valid_author_submit.json\") as f:\n",
    "    submission=json.load(f)\n",
    "    \n",
    "    \n",
    "labels=[]\n",
    "for e1, (id, person_info) in enumerate(train_author.items()):\n",
    "    for e2, text_id in enumerate(person_info['normal_data']): # 正样本\n",
    "        labels.append(1)\n",
    "    for text_id in person_info['outliers']:#负样本\n",
    "        labels.append(0)  \n",
    "        \n",
    "print(pd.Series(labels).value_counts(),\"\\n\",\n",
    "      pd.Series(labels).value_counts()[0] / pd.Series(labels).value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439cf582-b694-4b80-bf77-5ef83c4dd714",
   "metadata": {},
   "source": [
    "## groupby特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6eee9-d5c6-4e46-a265-4427d448d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in feat_col+[\"org_number\"]:\n",
    "    for m in ['count', 'sum', \"max\", \"min\", \"mean\", \"std\", \"median\",\"var\"]:\n",
    "        tmp = data.groupby(['year', \"venue\"])[col].agg(m).to_frame(name=f'{col}_venue_year_{m}').reset_index()\n",
    "        data = data.merge(tmp, on=['year', \"venue\"], how='left')\n",
    "        \n",
    "for col in feat_col+[\"org_number\"]:\n",
    "    for m in ['count', 'sum', \"max\", \"min\", \"mean\", \"std\", \"median\",\"var\"]:\n",
    "        tmp = data.groupby(['auth_id', \"venue\"])[col].agg(m).to_frame(name=f'{col}_auth_id_venue_{m}').reset_index()\n",
    "        data = data.merge(tmp, on=['auth_id', \"venue\"], how='left')\n",
    "        \n",
    "for col in [\"year\"]:\n",
    "    for m in [\"max\", \"min\", \"mean\", \"std\", \"median\",\"var\"]:\n",
    "        tmp = data.groupby([\"venue\"])[col].agg(m).to_frame(name=f'{col}_venueyear_{m}').reset_index()\n",
    "        data = data.merge(tmp, on=[\"venue\"], how='left')\n",
    "        \n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
